{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setuptools.dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('Coles_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load DistilBERT tokenizer and model\n",
    "pretrained_weights = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(pretrained_weights)\n",
    "bert_model = TFDistilBertModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract product names for tokenization\n",
    "product_names = df['item_name'].fillna(\"\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for product names\n",
    "product_embeddings = []\n",
    "for name in product_names:\n",
    "    # Tokenize product names\n",
    "    tokenized = tokenizer.encode(name, add_special_tokens=True, max_length=20, truncation=True)\n",
    "    padded = tokenized + [0] * (20 - len(tokenized))  # Pad to max length of 20\n",
    "    input_tensor = tf.convert_to_tensor([padded])  # Convert to tensor\n",
    "    \n",
    "    # Pass through DistilBERT model\n",
    "    output = bert_model(input_tensor)\n",
    "    embedding = tf.reduce_mean(output.last_hidden_state, axis=1).numpy()  # Mean pooling\n",
    "    product_embeddings.append(embedding[0])\n",
    "\n",
    "product_embeddings = np.array(product_embeddings)  # Convert to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 32  # Number of product names per batch\n",
    "embedding_save_path = \"product_embeddings\"  # Directory to save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (20608, 768)\n",
      "Embeddings saved in folder: product_embeddings\n"
     ]
    }
   ],
   "source": [
    "product_embeddings = []\n",
    "for i in range(0, len(product_names), batch_size):\n",
    "    # Get the current batch\n",
    "    batch = product_names[i:i + batch_size]\n",
    "    \n",
    "    # Tokenize the batch\n",
    "    tokenized_batch = tokenizer(batch, padding=True, truncation=True, max_length=20, return_tensors=\"tf\")\n",
    "    \n",
    "    # Generate embeddings using DistilBERT\n",
    "    output = bert_model(tokenized_batch['input_ids'])\n",
    "    mean_pooled = tf.reduce_mean(output.last_hidden_state, axis=1).numpy()  # Mean pooling\n",
    "\n",
    "    # Append to the embeddings list\n",
    "    product_embeddings.extend(mean_pooled)\n",
    "\n",
    "    # Save each batch inside the folder\n",
    "    batch_file_path = os.path.join(embedding_save_path, f\"batch_{i // batch_size}.npy\")\n",
    "    np.save(batch_file_path, mean_pooled)\n",
    "\n",
    "# Convert all embeddings to a NumPy array\n",
    "product_embeddings = np.array(product_embeddings)\n",
    "\n",
    "# Save all embeddings to a single file inside the folder\n",
    "all_embeddings_file_path = os.path.join(embedding_save_path, \"all_embeddings.npy\")\n",
    "np.save(all_embeddings_file_path, product_embeddings)\n",
    "\n",
    "# Load saved embeddings (for reuse)\n",
    "loaded_embeddings = np.load(all_embeddings_file_path)\n",
    "\n",
    "print(\"Embeddings shape:\", loaded_embeddings.shape)\n",
    "print(f\"Embeddings saved in folder: {embedding_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_embeddings = np.load(\"product_embeddings/all_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "similarity_matrix = cosine_similarity(product_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a recommendation function\n",
    "def recommend_products(product_name, top_n=5):\n",
    "    # Find the index of the input product\n",
    "    if product_name not in product_names:\n",
    "        return f\"Product '{product_name}' not found in the dataset.\"\n",
    "    \n",
    "    idx = product_names.index(product_name)\n",
    "    # Get indices of top N similar products (excluding the input product itself)\n",
    "    similar_indices = np.argsort(similarity_matrix[idx])[::-1][1:top_n + 1]\n",
    "    # Fetch product names based on indices\n",
    "    recommendations = [product_names[i] for i in similar_indices]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for 'Coles Boneless Pork Leg Roast | approx 2.1kg': ['Coles Boneless Pork Shoulder Roast | approx 2.6kg', 'Coles Pork Belly Roast Boneless | approx 1.3kg', 'Coles Lamb Boneless Shoulder Roast | approx 1.3kg', 'Coles Butcher Lamb Leg Roast Boneless | approx 1.08kg', 'Coles Lamb Whole Lamb Leg Roast | approx 2.8kg']\n"
     ]
    }
   ],
   "source": [
    "# Example recommendation for the first product\n",
    "example_product = product_names[0]\n",
    "recommendations = recommend_products(example_product, top_n=5)\n",
    "print(f\"Recommendations for '{example_product}': {recommendations}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
